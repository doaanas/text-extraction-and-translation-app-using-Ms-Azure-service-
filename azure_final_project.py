# -*- coding: utf-8 -*-
"""Azure_Final_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nqSEXS4IbiDAVLPRyohxVmC0hJjlsXOY

### **Virtual Document Translator**

## **Here the OCR extracts text from detected paper**
"""

!pip install azure-ai-vision-imageanalysis==1.0.0b3

!pip install azure-ai-textanalytics==5.3.0

!pip install python-dotenv

!pip install azure-ai-translation-text==1.0.0b1

!pip install azure-cognitiveservices-speech

from dotenv import load_dotenv
import os
import time
from PIL import Image, ImageDraw
from matplotlib import pyplot as plt
import requests, json

# Import namespaces
from azure.ai.vision.imageanalysis import ImageAnalysisClient
from azure.ai.vision.imageanalysis.models import VisualFeatures
from azure.core.credentials import AzureKeyCredential
from azure.ai.translation.text import *
from azure.ai.translation.text.models import InputTextItem

import azure.cognitiveservices.speech as speechsdk
from IPython.display import display, Audio


def main():

    global cv_client
    global translator_endpoint
    global cog_key
    global cog_region
    global client
    global text
    global language
    global translation
    repeat = True


    while repeat:
        try:
           # Get Configuration Settings
           load_dotenv()
           ai_endpoint = "https://final1.cognitiveservices.azure.com/"
           ai_key = "8de58bdd42e54f7499c7c28a7b9e2622"

           cog_key = 'dd4c8df8d8ee4176878a260c7ccca826'
           cog_region ='eastus'
           translator_endpoint = 'https://api.cognitive.microsofttranslator.com'

           #Create client using endpoint and key
           credential = TranslatorCredential(cog_key, cog_region)
           client = TextTranslationClient(credential)

           # Authenticate Azure AI Vision client
           cv_client = ImageAnalysisClient(
           endpoint=ai_endpoint,
           credential=AzureKeyCredential(ai_key)
            )

           # Menu for text reading functions
           print('\n1: Use Read API for image (error.jpg)\n2: Read handwriting (ITI.jpg)\nAny other key to quit\n')
           command = input('Enter a number:')
           if command == '1':
                image_file = ('/content/error.PNG')
                GetTextRead(image_file)
           elif command =='2':
                image_file = ('/content/ITI')
                GetTextRead(image_file)

           # Access the latest created text file
           Content_folder = 'Content_folder'
           all_files = os.listdir(Content_folder)
           # Find the latest file (assuming files are named with timestamps)
           latest_file = max(all_files, key=lambda f: os.path.getctime(os.path.join(Content_folder, f)))

           # Read the file contents
           print('\n-------------\n' + latest_file)
           text = open(os.path.join(Content_folder, latest_file), encoding='utf8').read()
           print('\n' + text)

           # Detect the language
           language = GetLanguage(text)
           print('Language:',language)

           # Translate
           translation = Translate(text, language)
           print("\nTranslation:\n{}".format(translation))

           #Speak up
           reply=input("Would you like to listen to Translated text?(yes/no)")
           if reply.lower() == "yes":
              speak(translation)
           repeat=input("would you like to translate another paper ?(yes/no)")
           if repeat.lower()=='no':
                break
           else:
               continue

        except Exception as ex:
            print(ex)

def GetTextRead(image_file):
    i = 1
    print('\n')

    # Open image file
    with open(image_file, "rb") as f:
        image_data = f.read()

    # Use Analyze image function to read text in image
    result = cv_client.analyze(
        image_data=image_data,
        visual_features=[VisualFeatures.READ]
    )

    # Display the image and overlay it with the extracted text
    if result.read is not None:
        #print("\nText:")

        # Prepare image for drawing
        image = Image.open(image_file)
        fig = plt.figure(figsize=(image.width/100, image.height/100))
        plt.axis('off')
        draw = ImageDraw.Draw(image)
        color = 'cyan'

         #Create a text file with the resulted text
        os.makedirs('Content_folder', exist_ok = True)
        with open(f"/content/Content_folder/output_{time.time()}.txt", "w") as f:
          for line in result.read.blocks[0].lines:
               # Return the text detected in the image
            #print(f"  {line.text}")


            r = line.bounding_polygon
            bounding_polygon = ((r[0].x, r[0].y),(r[1].x, r[1].y),(r[2].x, r[2].y),(r[3].x, r[3].y))


            # Draw line bounding polygon
            draw.polygon(bounding_polygon, outline=color, width=3)




            # Iterate over recognized lines of text

            # Write each line to the file
            f.write(line.text)
            # Add a newline character after each line
            f.write("\n")


    # Save image
    plt.imshow(image)
    plt.tight_layout(pad=0)
    outputfile = 'text.jpg'
    fig.savefig(outputfile)
    print('\n  Results saved in', outputfile)

def GetLanguage(text):
    #Use the Azure AI Translator detect function
    # Choose target language
    languagesResponse = client.get_languages(scope="translation")
    #print("{} languages supported.".format(len(languagesResponse.translation)))
    print("Enter a target language code for translation (for example, 'fr'):")
    targetLanguage = "xx"
    supportedLanguage = False
    while supportedLanguage == False:
       targetLanguage = input()
       if  targetLanguage in languagesResponse.translation.keys():
           supportedLanguage = True
       else:
        print("{} is not a supported language.".format(targetLanguage))

    # Return the language
    return targetLanguage

def Translate(text, targetLanguage):

    # Use the Azure AI Translator translate function
    input_text_elements = [InputTextItem(text=text)]
    translationResponse = client.translate(content=input_text_elements, to=[targetLanguage])
    translation = translationResponse[0].translations[0].text if translationResponse else None

    # Return the translation
    return translation

def speak(text):
      # This example requires environment variables named "SPEECH_KEY" and "SPEECH_REGION"
      speech_config = speechsdk.SpeechConfig(subscription='2b9659352e704d1fbaab2fc10de5e226', region='eastus')
      audio_config = speechsdk.audio.AudioOutputConfig(use_default_speaker=True)

     # The neural multilingual voice can speak different languages based on the input text.
      speech_config.speech_synthesis_voice_name='en-US-AvaMultilingualNeural'

      speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)


      speech_synthesis_result = speech_synthesizer.speak_text_async(text).get()
      stream = speechsdk.AudioDataStream(speech_synthesis_result)
      stream.save_to_wav_file("speech.wav")

      if speech_synthesis_result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:
         print("Speech synthesized for text [{}]".format(text))
         display(Audio(filename="speech.wav", autoplay=True))

      elif speech_synthesis_result.reason == speechsdk.ResultReason.Canceled:
        cancellation_details = speech_synthesis_result.cancellation_details
        if cancellation_details is not None:
           print("Speech synthesis canceled: {}".format(cancellation_details.reason))
           if cancellation_details.reason == speechsdk.CancellationReason.Error:
              if cancellation_details.error_details:
                print("Error details: {}".format(cancellation_details.error_details))
                print("Did you set the speech resource key and region values?")


if __name__ == "__main__":
    main()

"""### **UI**"""

!pip install ipywidgets==8.0.6

import ipywidgets as widgets
from IPython.display import display
import io
from PIL import Image
import time

# ... (rest of your existing code)

# Create a file upload widget
uploader = widgets.FileUpload(
    accept='image/*',
    multiple=False
)

# Create a dropdown for language selection
language_dropdown = widgets.Dropdown(
    options=['fr', 'es', 'de', 'it','en'],
    description='Target Language:'
)

# Create a button to trigger translation
translate_button = widgets.Button(description="Translate")

# Create a checkbox for listening to the translation
listen_checkbox = widgets.Checkbox(
    value=False,
    description='Listen to Translation',
    disabled=False,
    indent=False
)

# Create an output widget to display results
output = widgets.Output()

# Define the on_click function for the button
def on_translate_button_clicked(b):
    with output:
        output.clear_output()
        if uploader.value:
            # ... (image processing and text extraction code remains the same)
            target_language = language_dropdown.value
            # Translate the text
            translation = Translate(text, target_language)
            print("\nTranslation:\n{}".format(translation))

            # Speak the translation if the checkbox is checked
            if listen_checkbox.value:
                speak(translation)

# Attach the on_click function to the button
translate_button.on_click(on_translate_button_clicked)

# Display the widgets
display(uploader, language_dropdown, translate_button, listen_checkbox, output)

"""**Streamlit**"""

!pip install streamlit --upgrade

! pip install streamlit -q
!wget -q -O - ipv4.icanhazip.com

! streamlit run Azure.py & npx localtunnel --port 8501